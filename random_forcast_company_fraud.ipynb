# -*- coding: utf-8 -*-
"""
Created on Mon Dec  4 20:14:11 2023

@author: ARK00
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
from sklearn.preprocessing import LabelEncoder

# Assuming 'df' is your DataFrame with the dataset
# Replace 'your_target_column' with the actual name of your target column

# Step 1: Data Preprocessing
# Convert the target variable 'Sales' into a categorical variable
df['Sales'] = pd.qcut(df['Sales'], q=5, labels=False)

# Handle missing values and encode categorical variables if needed

# Extract features (X) and target variable (y)
X = df.drop('Sales', axis=1)
y = df['Sales']

# Encode categorical variables if needed
le = LabelEncoder()
X_encoded = X.apply(le.fit_transform)

# Step 2: Split Data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Step 3: Random Forest Model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Step 4: K-fold Cross-Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(rf_model, X_train, y_train, cv=kfold, scoring='accuracy')  # Use appropriate scoring metric
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

# Step 5: Bagging Regressor
bagging_model = BaggingRegressor(base_estimator=rf_model, n_estimators=10, random_state=42)
bagging_scores = cross_val_score(bagging_model, X_train, y_train, cv=kfold, scoring='accuracy')  # Use appropriate scoring metric
print("Bagging Accuracy: %0.2f (+/- %0.2f)" % (bagging_scores.mean(), bagging_scores.std() * 2))

# Step 6: Feature Importance
rf_model.fit(X_train, y_train)
feature_importances = rf_model.feature_importances_
print("Feature Importances:", feature_importances)

# Step 7: Model Evaluation on the Test Set
rf_model.fit(X_train, y_train)
test_accuracy = rf_model.score(X_test, y_test)  # Use appropriate metric
print("Test Accuracy:", test_accuracy)


===============================================================================
#fraud_check

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load your fraud data (replace 'your_data.csv' with your actual dataset)
df = pd.read_csv('fraud_check.csv')
df.head()

df['Risk'] = df['Taxable.Income'].apply(lambda x: 'Risky' if x <= 30000 else 'Good')

# Drop the original 'taxable_income' column if not needed
df.drop('Taxable.Income', axis=1, inplace=True)


X = df.drop('Risk', axis=1)
y = df['Risk']

# Encode categorical variables if needed
le = LabelEncoder()
X_encoded = X.apply(le.fit_transform)
y_encoded = le.fit_transform(y)

# Step 2: Split Data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)

# Step 3: Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Step 4: K-fold Cross-Validation
kfold = KFold


kfold = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(rf_model, X_train, y_train, cv=kfold, scoring='accuracy')
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

# Step 5: Bagging Classifier
bagging_model = BaggingClassifier(base_estimator=rf_model, n_estimators=10, random_state=42)
bagging_scores = cross_val_score(bagging_model, X_train, y_train, cv=kfold, scoring='accuracy')
print("Bagging Accuracy: %0.2f (+/- %0.2f)" % (bagging_scores.mean(), bagging_scores.std() * 2))

rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))
This code assumes that you have a column named 'taxable_income' in your dataset and creates a new column 'Income_Category' based on the condition specified. The Random Forest model is trained, and k-fold cross-validation is performed. The Bagging Classifier is then applied for further ensemble learning. Finally, the model is evaluated on the test set using confusion matrix, classification report, and accuracy score. Adjust the code according to the specifics of your dataset and requirements.















