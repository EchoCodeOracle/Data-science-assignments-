# -*- coding: utf-8 -*-
"""
Created on Fri Dec 15 14:45:24 2023

@author: ARK00
"""
#company_data
pip install lightgbm

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

data = pd.read_csv('company_data.csv')

target_variable = 'Sales'

le = LabelEncoder()
data[target_variable] = le.fit_transform(data[target_variable])

X = data.drop(target_variable, axis=1)
y = data[target_variable]

X_encoded = pd.get_dummies(X)

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)

print("Random Forest Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(le.classes_))

xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)

print("\nXGBoost Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

lgb_model = lgb.LGBMClassifier()

lgb_model.fit(X_train, y_train)

y_pred_lgb = lgb_model.predict(X_test)

print("\nLightGBM Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_lgb))
print("Classification Report:\n", classification_report(y_test, y_pred_lgb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lgb))



===============================================================================
#fraud_check

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('fraud_check.csv')
df.head()
list(df)

df['Risk'] = df['Taxable.Income'].apply(lambda x: 'Risky' if x <= 30000 else 'Good')

df.drop('Taxable.Income', axis=1, inplace=True)
y=df["Urban"]
x=df["City.Population"]

import matplotlib.pyplot as plt
plt.scatter(df["Marital.Status"],df["Urban"],color='black')
plt.title('fraud check')
plt.xlabel('Marital.Status')
plt.ylabel('Urban')
plt.grid(True)
plt.show()

X = df.drop('Risk', axis=1)
y = df['Risk']

le = LabelEncoder()
X_encoded = X.apply(le.fit_transform)
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

kfold = KFold


kfold = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(rf_model, X_train, y_train, cv=kfold, scoring='accuracy')
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

# Step 5: Bagging Classifier
bagging_model = BaggingClassifier(base_estimator=rf_model, n_estimators=10, random_state=42)
bagging_scores = cross_val_score(bagging_model, X_train, y_train, cv=kfold, scoring='accuracy')
print("Bagging Accuracy: %0.2f (+/- %0.2f)" % (bagging_scores.mean(), bagging_scores.std() * 2))

rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))
This code assumes that you have a column named 'taxable_income' in your dataset and creates a new column 'Income_Category' based on the condition specified. The Random Forest model is trained, and k-fold cross-validation is performed. The Bagging Classifier is then applied for further ensemble learning. Finally, the model is evaluated on the test set using confusion matrix, classification report, and accuracy score. Adjust the code according to the specifics of your dataset and requirements.
   

